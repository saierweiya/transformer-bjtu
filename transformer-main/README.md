# ğŸ“– é¡¹ç›®ç®€ä»‹

æœ¬ä»“åº“åŒ…å«ä¸€ä¸ªç”¨äºåœ¨ IWSLT2017 æ•°æ®é›†ï¼ˆè‹±è¯­åˆ°å¾·è¯­ï¼‰ä¸Šè¿›è¡Œæœºå™¨ç¿»è¯‘çš„ Transformer æ¨¡å‹çš„å®ç°ã€‚
ä¸€ä¸ªä»é›¶å®ç°çš„Transformeræ¨¡å‹ï¼ŒåŒ…å«å®Œæ•´çš„è®­ç»ƒæ¡†æ¶ã€æ¶ˆèå®éªŒå’Œå¯è§†åŒ–å·¥å…·ã€‚

## âœ¨æ ¸å¿ƒç‰¹æ€§

- ğŸ—ï¸ å®Œæ•´Transformerå®ç°ï¼šç¼–ç å™¨-è§£ç å™¨æ¶æ„ã€å¤šå¤´æ³¨æ„åŠ›ã€ä½ç½®ç¼–ç 
- ğŸ”¬ æ¶ˆèå®éªŒæ”¯æŒï¼šæ— ä½ç½®ç¼–ç ã€å•å¤´æ³¨æ„åŠ›ã€æ— å±‚å½’ä¸€åŒ–ã€æ— æ®‹å·®è¿æ¥
- ğŸ“Š å¯è§†åŒ–å·¥å…·ï¼šè®­ç»ƒæ›²çº¿ã€æ€§èƒ½æ›²çº¿å¯¹æ¯”
- âš¡ é«˜æ•ˆè®­ç»ƒï¼šå­¦ä¹ ç‡è°ƒåº¦
- ğŸ§ª å®éªŒç®¡ç†ï¼šå®Œæ•´çš„å®éªŒè®°å½•å’Œç»“æœåˆ†æ



## ç¯å¢ƒå®‰è£…

1.  **å…‹éš†ä»“åº“ï¼š**
    ```bash
    git clone https://github.com/saierweiya/transformer-bjtu.git
    cd transformer-main
    ```

2.  **åˆ›å»º Python ç¯å¢ƒï¼ˆæ¨èï¼‰ï¼š**
    ```bash
    conda create -n transformer_env python=3.10
    conda activate transformer_env
    ```

3.  **å®‰è£…ä¾èµ–ï¼š**
    ```bash
    pip install -r requirements.txt
    ```

## æ•°æ®å‡†å¤‡

ä¿®æ”¹è¿è¡Œè„šæœ¬æˆ–å‘½ä»¤è¡Œä¸­çš„ `--data_path` å‚æ•°ã€‚

## ğŸ§  æ¨¡å‹æ¶æ„
æ ‡å‡†Transformeré…ç½®
ç»„ä»¶	é…ç½®
æ¨¡å‹ç»´åº¦ (d_model)	512
æ³¨æ„åŠ›å¤´æ•° (nhead)	8
ç¼–ç å™¨å±‚æ•°	6
è§£ç å™¨å±‚æ•°	6
å‰é¦ˆç½‘ç»œç»´åº¦	2048
Dropout	0.1

##ğŸš€ ä¸€é”®è¿è¡Œæ‰€æœ‰å®éªŒ

```bash
bash scripts/run.sh
```

1.  è®­ç»ƒå®Œæ•´ Transformer æ¨¡å‹ï¼ˆé»˜è®¤å‚æ•°ï¼š`d_model=512`, `nhead=8`, `num_encoder_layers=6`, `num_decoder_layers=6`, `dim_feedforward=2048`ï¼‰ã€‚
2.  è®­ç»ƒå°å‹ Transformer æ¨¡å‹ï¼ˆå‚æ•°ï¼š`d_model=256`, `nhead=4`, `num_encoder_layers=3`, `num_decoder_layers=3`, `dim_feedforward=1024`ï¼‰ã€‚
3.  è®­ç»ƒå°å‹ Transformer æ¨¡å‹çš„æ¶ˆèå®éªŒï¼ˆæ— æ®‹å·®è¿æ¥ï¼‰ã€‚
4.  è®­ç»ƒå°å‹ Transformer æ¨¡å‹çš„æ¶ˆèå®éªŒï¼ˆæ— ä½ç½®ç¼–ç ï¼‰ã€‚
5.  è®­ç»ƒå°å‹ Transformer æ¨¡å‹çš„æ¶ˆèå®éªŒï¼ˆæ— å¤šå¤´æ³¨æ„åŠ›ï¼‰ã€‚
6.  è®­ç»ƒå°å‹ Transformer æ¨¡å‹çš„æ¶ˆèå®éªŒï¼ˆæ— å±‚å½’ä¸€åŒ–ï¼‰ã€‚
7.  åœ¨ `./src/results/` ç›®å½•ä¸‹ç”Ÿæˆä¸€ä¸ªæ¶ˆèå®éªŒå¯¹æ¯”å›¾ï¼ˆ`ablation_comparison.png`ï¼‰ã€‚
8.  åœ¨ `./src/results/` ç›®å½•ä¸‹ç”Ÿæˆä¸€ä¸ªæ‰€æœ‰æ¨¡å‹å¯¹æ¯”å›¾ï¼ˆ`all_models_comparison.png`ï¼‰ã€‚
9.  åœ¨ `./src/results/` ç›®å½•ä¸‹ç”Ÿæˆæ‰€æœ‰æ¨¡å‹æ•°æ®å›¾ï¼ˆ`all_models_metrics.csv`ï¼‰ã€‚
10. åœ¨ `./src/results/` ç›®å½•ä¸‹ç”Ÿæˆå¤§å°æ¨¡å‹å¯¹æ¯”å›¾ï¼ˆ`size_comparison.png`ï¼‰ã€‚

## ğŸ“ é¡¹ç›®ç»“æ„

```
â”œâ”€â”€ data  
â”‚   â””â”€â”€ de-en 
â”‚       â”œâ”€â”€ train.tags.de-en.de     
â”‚       â”œâ”€â”€ train.tags.de-en.en    
â”‚       â””â”€â”€ DatasetREADME.md 
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ Transformer.py     
â”‚   â”œâ”€â”€ dataloader.py     
â”‚   â”œâ”€â”€ trainer.py              
â”‚   â”œâ”€â”€ main.py                
â”‚   â””â”€â”€ draw.py         
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ run.sh                 
â”œâ”€â”€ results/
â”‚   â”œâ”€â”€ standard_Transformer/transformer_large_training_history.csv     
â”‚   â”œâ”€â”€ Small_Transformer/transformer_small_full_full_training_history.csv     
â”‚   â”œâ”€â”€ no_residual_Small_Transformer/transformer_small_full_no_residual_training_history.csv              
â”‚   â”œâ”€â”€ results/no_layer_norm_Small_Transformer/transformer_small_full_no_layer_norm_training_history.csv      
â”‚   â”œâ”€â”€ no_positional_encoding_Small_Transformer/transformer_small_full_no_positional_encoding_training_history.csv              
â”‚   â””â”€â”€ single_head_Small_Transformer/transformer_small_full_full_training_history.csv                   
â”œâ”€â”€ requirements.txt           
â””â”€â”€ README.md                   
```

## ğŸ“ˆ å®éªŒç»“æœ
| æ¨¡å‹å˜ä½“          | éªŒè¯æŸå¤± | è®­ç»ƒæ—¶é—´ | å‚æ•°é‡ |
|------------------|----------|----------|--------|
| Transformer-Large | 2.15     | 45 min   | 65 M   |
| Transformer-Small | 2.89     | 25 min   | 18 M   |
| æ— ä½ç½®ç¼–ç         | 3.42     | 42 min   | 65 M   |
| å•å¤´æ³¨æ„åŠ›        | 2.78     | 40 min   | 65 M   |

